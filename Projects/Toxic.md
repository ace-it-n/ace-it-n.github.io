<h1 style="background-color:BlanchedAlmond;font-family:Candara;">Toxic Comment Classification using Natural Language Processing</h1>

<a href="https://github.com/ace-it-n/Projects/blob/master/Multiple%20Linear%20Regression%20on%20Advertising%20Dataset/Multiple%20Linear%20Regression%20on%20Advertising%20Dataset.ipynb" target="_blank">Jupyter Notebook on Github</a>

<h2 style="background-color:Aquamarine;font-family:Candara;">Project Description</h2>

About the Dataset:
Source - [Toxic Comment Classification Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)

You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:

- toxic
- severe_toxic
- obscene
- threat
- insult
- identity_hate

You must create a model which predicts a probability of each type of toxicity for each comment.

- LSTM RNN Model achieved accuracy of 98%
